import json
import requests
import base64
from PIL import Image
from io import BytesIO
import time
import os
import sys
import shutil
import openai
from gradio_client import Client
from numpy.random import randint

openai.api_type = "azure"
openai.api_base = "https://ura-gpt4-v.openai.azure.com/"
openai.api_version = "2023-07-01-preview"
openai.api_key = "3cb748f82bb04208aa84734bae93959d"

GPT4V = [
    {
        'key': "3cb748f82bb04208aa84734bae93959d",
        'endpoint': "https://ura-gpt4-v.openai.azure.com/openai/deployments/gpt4-v/chat/completions?api-version=2023-07-01-preview"
    },
    {
        'key': "608425edbd604db88f2f62967e599077",
        'endpoint': "https://ura-gpt4-v-1.openai.azure.com/openai/deployments/gpt4-v/chat/completions?api-version=2023-07-01-preview"
    },
    {
        # This api is slow
        'key': "f836e2c8914e4fdb9f3270fde74068bd",
        'endpoint': "https://ura-gpt4-v-2.openai.azure.com/openai/deployments/ura-gpt4-v/chat/completions?api-version=2023-07-01-preview"
    }
]

LLAVA_URL = 'http://localhost:11434/api/generate'
LLAVA_13B_MODEL = 'llava:13b-v1.6-vicuna-q4_K_M'
LLAVA_7B_MODEL = 'llava:7b-v1.6-vicuna-q4_K_M'
COGVLM_URL = "https://e43763510c939e3939.gradio.live"

def getImageFromSD(prompt, SD_URL):
    """
    Retrieves a base64 image from stable diffusion local server based on the provided prompt. 

    Parameters:
    - prompt (str): The user prompt/description to be used as input to generate image.
    
    Returns:
    - str (base 64 encoded): The image generated encoded to base 64.
    """
    start_time = time.time()
        
    sdRequestBody = {'prompt': prompt}
    sdResponse = requests.post(SD_URL, data=sdRequestBody)
    image = sdResponse.json()['image']
    
    process_time = time.time() - start_time

    return image, process_time

def getResponeFromLLaVA13b(target, image=None):
    """
    Retrieves a response from the LLaVa 13b multimodal model based on the provided target prompt and optional image. 

    Parameters:
    - target (str): The user prompt to be used as input to the model. It typically includes a request for evaluation along with a description.
    - image (str, optional): A base64 encoded image to be evaluated alongside the target prompt. If provided, the model will assess whether the image aligns with the description in the target.

    Returns:
    - str: A response generated by the model, indicating its evaluation based on the input target prompt and image.
    """
    if image:
        llavaRequestBody = {
            'model': LLAVA_13B_MODEL, 
            'prompt': target, # string
            'images': [image] # base64 string
        }
        message = "[Evaluating]"
        
    else:
        llavaRequestBody = {
            'model': LLAVA_13B_MODEL, 
            'prompt': target, # string
        }
        message = "[Improving prompt]"
        
        
    return_prompt = ""

    print(f"\t{message}...")
    start_time = time.time()
        
    llavaResponse = requests.post(LLAVA_URL, data=json.dumps(llavaRequestBody))
    # print(llavaResponse.text)
        
    for line in llavaResponse.iter_lines():
        if line:  # filter out keep-alive new lines
            # print('[LINE]', line)
            json_line = json.loads(line.decode('utf-8'))
            if 'response' in json_line:
                return_prompt += json_line['response']
        
    llavaResultMessage = return_prompt.strip()
    process_time = time.time() - start_time
    print(f"\t[Result] in {process_time:0.1f} secs: ", llavaResultMessage)
    return llavaResultMessage, process_time


def getResponeFromLLaVA7b(target, image=None):
    """
    Retrieves a response from the LLaVa 7b multimodal model based on the provided target prompt and optional image. 

    Parameters:
    - target (str): The user prompt to be used as input to the model. It typically includes a request for evaluation along with a description.
    - image (str, optional): A base64 encoded image to be evaluated alongside the target prompt. If provided, the model will assess whether the image aligns with the description in the target.

    Returns:
    - str: A response generated by the model, indicating its evaluation based on the input target prompt and image.
    """
    if image:
        llavaRequestBody = {
            'model': LLAVA_7B_MODEL, 
            'prompt': target, # string
            'images': [image] # base64 string
        }
        message = "[Evaluating]"
        
    else:
        llavaRequestBody = {
            'model': LLAVA_7B_MODEL, 
            'prompt': target, # string
        }
        message = "[Improving prompt]"
        
        
    return_prompt = ""

    print(f"\t{message}...")
    start_time = time.time()
        
    llavaResponse = requests.post(LLAVA_URL, data=json.dumps(llavaRequestBody))
    # print(llavaResponse.text)
        
    for line in llavaResponse.iter_lines():
        if line:  # filter out keep-alive new lines
            # print('[LINE]', line)
            json_line = json.loads(line.decode('utf-8'))
            if 'response' in json_line:
                return_prompt += json_line['response']
        
    llavaResultMessage = return_prompt.strip()
    process_time = time.time() - start_time
    print(f"\t[Result] in {process_time:0.1f} secs: ", llavaResultMessage)
    return llavaResultMessage, process_time

def getResponeFromGPT4V(target, image=None):
    """
    Retrieves a response from the GPT4-Vision multimodal model based on the provided target prompt and optional image. 

    Parameters:
    - target (str): The user prompt to be used as input to the model. It typically includes a request for evaluation along with a description.
    - image (str, optional): A base64 encoded image to be evaluated alongside the target prompt. If provided, the model will assess whether the image aligns with the description in the target.

    Returns:
    - str: A response generated by the model, indicating its evaluation based on the input target prompt and image.
    """
    if image:
        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": "You are an AI assistant that helps people find information."
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image}"
                            }
                        },
                        {
                            "type": "text",
                            "text": target
                        }
                    ]
                }
            ],
            "temperature": 0.7,
            "top_p": 0.95,
            "max_tokens": 800
        }
        message = "[Evaluating]"
        
    else:
        payload = {
            "messages": [
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": "You are an AI assistant that helps people find information."
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": target
                        }
                    ]
                }
            ],
            "temperature": 0.7,
            "top_p": 0.95,
            "max_tokens": 800
        }
        message = "[Improving prompt]"
    
    n = randint(2) # third API is slow so use just the first two.
        
    headers = {
        "Content-Type": "application/json",
        "api-key": GPT4V[n]['key'],
    }

    print(f"\t{message}(using api [{n+1}])...")
    # print(target)
    start_time = time.time()
    
    # try:
    gptResponse = requests.post(GPT4V[n]['endpoint'], headers=headers, json=payload)
    # Will raise an HTTPError if the HTTP request returned an unsuccessful status code
    gptResponse.raise_for_status()
    # except requests.RequestException as e:
    #     raise SystemExit(f"Failed to make the request. Error: {e}")

    # Handle the response as needed (e.g., print or process)
    # print(response.json()['choices'][0]['message']['content'])
    # gptResponse = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)
    # print(llavaResponse.text)
        
    gptResultMessage = gptResponse.json()['choices'][0]['message']['content']
    process_time = time.time() - start_time
    print(f"\t[Result] in {process_time:0.1f} secs: ", gptResultMessage)
    return gptResultMessage, process_time

def getResponeFromGPT4(target):
    """
    Retrieves a response from the GPT4 model based on the provided target prompt.

    Parameters:
    - target (str): The user prompt to be used as input to the model. It should include a request for response generation along with any context or description.

    Returns:
    - str: A response generated by the GPT4 model based on the input target prompt.
    """
    message_text = [{"role": "system", "content": "You are an AI assistant that helps people find information."}, {
    "role": "user", "content": target}]
    message = "[Improving prompt]"

    print(f"\t{message}...")
    # print(target)
    start_time = time.time()
    gptResponse = openai.ChatCompletion.create(
        engine="gpt4",
        messages=message_text,
        temperature=0.7,
        max_tokens=800,
        top_p=0.95,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None
    )

    # Handle the response as needed (e.g., print or process)
    # print(response.json()['choices'][0]['message']['content'])
    # gptResponse = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)
    # print(llavaResponse.text)
        
    gptResultMessage = gptResponse['choices'][0]['message']['content']
    process_time = time.time() - start_time
    print(f"\t[Result] in {process_time:0.1f} secs: ", gptResultMessage)
    return gptResultMessage, process_time

def getResponeFromCOGVLM(target, image="https://upload.wikimedia.org/wikipedia/commons/3/38/Solid_white_bordered.png"):
    """
    Retrieves a response from the CogVLM multimodal model based on the provided target prompt and optional image. 

    Parameters:
    - target (str): The user prompt to be used as input to the model. It typically includes a request for evaluation along with a description.
    - image (str, optional): A path to the image to be evaluated alongside the target prompt. If provided, the model will assess whether the image aligns with the description in the target.

    Returns:
    - str: A response generated by the model, indicating its evaluation based on the input target prompt and image.
    """
    
    if image == "https://upload.wikimedia.org/wikipedia/commons/3/38/Solid_white_bordered.png":
        temperature = 0.6
        top_p = 0.8
        top_k = 100
        message = "[Improving prompt]"
        
    else:
        temperature = 0.5
        top_p = 0.5
        top_k = 10
        message = "[Evaluating]"
    
    print(f"\t{message}...")
    start_time = time.time()

    client = Client(COGVLM_URL)
    result = client.predict(
            target,	# str  in 'Input Text' Textbox component
            temperature,	# float (numeric value between 0 and 1) in 'Temperature' Slider component
            top_p,	# float (numeric value between 0 and 1) in 'Top P' Slider component
            top_k,	# float (numeric value between 1 and 100) in 'Top K' Slider component
            image,	# filepath  in 'Image Prompt' Image component
            [["Hello, please answer the below question, command as short as possible", "I will response as short as possible, but still giving you the best answer"]],	# [[Question, Answer]]
            "",	# str  in 'parameter_22' Textbox component
            api_name="/post"
    )

    # print(result)
    cogvlmResultMessage = result[1][-1][1] # Ans <- (..., [[..., ...], ..., [Ques, Ans]], ...)

    process_time = time.time() - start_time
    print(f"\t[Result] in {process_time:0.1f} secs: ", cogvlmResultMessage)

    return cogvlmResultMessage, process_time

